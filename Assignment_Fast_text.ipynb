{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "15VoYBdAGOlj",
        "2r2olq635XJ8",
        "EuUWu8AS5F30",
        "ddQS5nPCyoYd",
        "75fgB4iEB2Jp"
      ],
      "authorship_tag": "ABX9TyMVWCrYoXgko047YUGrQ0Cq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swap1984/swapnil/blob/main/Assignment_Fast_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** Fast text embedding technique**"
      ],
      "metadata": {
        "id": "15VoYBdAGOlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastText is an advanced word embedding model developed by Facebookâ€™s AI Research (FAIR) team. Unlike Word2Vec, which learns vectors for entire words, FastText breaks down words into subwords (n-grams) and learns embeddings for these subword components. This makes FastText more robust to out-of-vocabulary (OOV) words and misspellings.\n",
        "\n",
        "**Advantages of FastText:**\n",
        "\n",
        "1)Handles Out-of-Vocabulary (OOV) Words:\n",
        "Since FastText learns from subwords, it can generate vectors for words not seen during training, unlike Word2Vec and GloVe, which fail on OOV words.\n",
        "\n",
        "2)Captures Morphological Information:\n",
        "FastText captures word morphology better by using character n-grams. This is especially useful in morphologically rich languages.\n",
        "\n",
        "3)Works Well on Small Datasets:\n",
        "Since it uses subword information, FastText often performs better on smaller datasets compared to models like Word2Vec.\n",
        "\n",
        "**Disadvantages of FastText:**\n",
        "\n",
        "1)Larger Memory Footprint:\n",
        "Due to the subword approach, FastText models can consume more memory and storage compared to Word2Vec and GloVe.\n",
        "\n",
        "2)Slower Training:\n",
        "Breaking down words into n-grams increases the number of computations required, leading to slower training compared to Word2Vec.\n",
        "\n",
        "3)Can Be Overly Sensitive to Misspellings:\n",
        "While FastText handles OOV words, it might over-rely on the subword information, potentially making it more sensitive to minor misspellings.\n",
        "\n",
        "**Applications of FastText:**\n",
        "\n",
        "1)Text Classification:\n",
        "FastText embeddings can improve text classification tasks by capturing subword-level information.\n",
        "\n",
        "2)Spell Checking and Autocorrect:\n",
        "FastText can identify and correct misspelled words based on subword similarities.\n",
        "\n",
        "3)Named Entity Recognition (NER):\n",
        "Helps in identifying entities even if they are slightly misspelled or in different forms.\n",
        "\n",
        "4)Multilingual Embeddings:\n",
        "FastText can be used for training multilingual embeddings, useful for translation tasks.\n"
      ],
      "metadata": {
        "id": "QCA6Zj7VGKKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison of FastText, Word2Vec, and GloVe in a tabular format based on various aspects**\n",
        "\n",
        "| **Aspect**                 | **FastText**                                 | **Word2Vec**                                | **GloVe**                                      |\n",
        "|----------------------------|----------------------------------------------|---------------------------------------------|------------------------------------------------|\n",
        "| **Model Type**              | Predictive (Subword information used)        | Predictive (Contextual)                     | Count-based (Global co-occurrence statistics)  |\n",
        "| **Subword Information**     | Yes (Handles subwords via n-grams)           | No (Word-level)                             | No (Word-level)                                |\n",
        "| **Out-of-Vocabulary (OOV)** | Handles OOV by creating vectors for subwords | Does not handle OOV words                   | Does not handle OOV words                      |\n",
        "| **Morphological Awareness** | High (Captures morphology)                   | Low (Fails with minor word variations)      | Low (Fails with minor word variations)         |\n",
        "| **Training Speed**          | Slower (Subword breakdown increases steps)   | Faster                                      | Requires pre-training (cannot retrain easily)  |\n",
        "| **Memory Requirement**      | High (Due to subword representations)        | Moderate                                    | Low (Pre-trained vectors, no subwords)         |\n",
        "| **Training Data Needed**    | Works well on small datasets                 | Needs larger datasets for good performance  | Pre-trained (on large corpus)                  |\n",
        "| **Handling Misspellings**   | Good (Generates embeddings for misspelled words) | Poor (Misspellings are treated as separate words) | Poor (Misspellings are treated as separate words) |\n",
        "| **Pre-trained Availability**| Available (but often requires custom training) | Available (but custom training required)    | Available (Pre-trained on large corpora)       |\n",
        "| **Embedding Type**          | Dynamic (Can be trained on new data)         | Dynamic (Can be trained on new data)        | Static (Pre-trained embeddings, cannot be fine-tuned) |\n",
        "| **Dimensionality**          | Adjustable during training                   | Adjustable during training                  | Fixed dimensionality (based on pre-trained vectors) |\n",
        "| **Contextual Information**  | Captures context with subword representations | Captures context with full words            | Captures global word co-occurrence statistics  |\n",
        "| **Common Applications**     | Text classification, spell check, NER        | Text classification, word similarity tasks  | Pre-trained for tasks like sentiment analysis  |\n"
      ],
      "metadata": {
        "id": "V0rpmLEbIceE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code for Fast text"
      ],
      "metadata": {
        "id": "2r2olq635XJ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SHnkjz41GJWV"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "from gensim.models import FastText\n",
        "from gensim.test.utils import common_texts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset (list of tokenized sentences)\n",
        "common_texts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9ZDabGOvEzX",
        "outputId": "4ad5b228-3e0b-4003-8fee-956f097a85c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human', 'interface', 'computer'],\n",
              " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
              " ['eps', 'user', 'interface', 'system'],\n",
              " ['system', 'human', 'system', 'eps'],\n",
              " ['user', 'response', 'time'],\n",
              " ['trees'],\n",
              " ['graph', 'trees'],\n",
              " ['graph', 'minors', 'trees'],\n",
              " ['graph', 'minors', 'survey']]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train FastText model\n",
        "model_ft = FastText(sentences=common_texts, vector_size=100, window=5, min_count=1, epochs=10)\n",
        "#sentences: The tokenized sentences to train the FastText model.\n",
        "#vector_size: Size of the embedding vector for each word.\n",
        "#window: Maximum distance between the current and predicted word in the sentence.\n",
        "#min_count: Minimum frequency of words to include in the vocabulary.\n",
        "#epochs: Number of training iterations."
      ],
      "metadata": {
        "id": "TA_RToY1vf17"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model_ft.save(\"fasttext.model\")"
      ],
      "metadata": {
        "id": "3UVpVKL5vurC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the vector for a word in the training set\n",
        "word_vector = model_ft.wv['computer']\n",
        "print(f\"Embedding for 'computer':\\n{word_vector}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZysDi7Mv_gK",
        "outputId": "ce50d117-89be-44c1-91c6-a8de164260d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for 'computer':\n",
            "[ 2.9691675e-04  3.3106157e-04 -8.7776285e-04  3.3971368e-04\n",
            " -5.0200790e-04 -2.0427848e-03 -1.2411864e-03 -1.9409786e-03\n",
            "  1.3460165e-03 -2.4136524e-03  9.1865542e-04 -1.0317172e-03\n",
            " -7.6370395e-04  7.3127791e-05  1.3834432e-03  5.1939470e-04\n",
            " -2.9898330e-04 -1.1951911e-03 -1.1728198e-03 -6.0902466e-04\n",
            " -6.7812472e-04  3.9287744e-04  9.9021803e-05  8.1278791e-04\n",
            "  5.8207958e-04  7.0247462e-04 -7.3670415e-04 -1.0399165e-03\n",
            " -6.2516180e-04 -2.4074930e-04 -1.1935978e-03 -2.6611326e-04\n",
            "  7.3650532e-04 -7.2191353e-04 -1.2753668e-03  1.2436805e-04\n",
            "  3.7794228e-04 -1.3318789e-03 -2.7351857e-03 -3.0493268e-04\n",
            "  9.2886400e-04 -7.2840165e-04 -1.1293935e-03 -3.2223188e-04\n",
            " -2.0584364e-04 -1.0493779e-04 -6.2300439e-04 -1.6142892e-03\n",
            "  9.9138310e-04  9.2263552e-05  3.6843625e-04 -5.3785514e-04\n",
            "  1.1336672e-03  8.7101565e-04 -1.6394290e-03 -8.5611606e-04\n",
            " -6.3161540e-04  6.2353048e-04  8.4044196e-04 -1.1285455e-03\n",
            "  1.2917615e-03 -3.4059497e-04 -1.1788004e-03 -1.6089173e-03\n",
            "  1.5278944e-03  3.0391884e-05 -2.4081828e-05 -7.2742492e-04\n",
            "  1.7338744e-03  8.9355971e-04  3.2681666e-04 -4.6365918e-04\n",
            " -2.3145417e-03 -1.7201898e-03  4.3608953e-04 -4.1219711e-04\n",
            " -1.0674229e-03 -1.0092477e-03 -1.6438538e-03 -1.0513966e-04\n",
            "  1.0195986e-03 -6.2455138e-04 -1.0819827e-03  8.8566908e-04\n",
            " -1.4578128e-03  6.4845535e-04  4.4167353e-04 -1.2452571e-03\n",
            "  3.4938217e-04 -9.8168640e-04 -9.7435270e-04 -1.9838198e-04\n",
            " -1.8973406e-04 -9.8557968e-04  5.7470007e-04  1.9909400e-03\n",
            "  7.2345632e-05  9.9593867e-04 -1.7091989e-03  1.3494290e-03]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling OOV word 'computr'\n",
        "if 'computr' in model_ft.wv:\n",
        "    print(f\"'computr' is in the vocabulary\")\n",
        "else:\n",
        "    print(f\"'computr' is not in the vocabulary but FastText can still generate a vector\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IodPGLazF4a",
        "outputId": "db3949e9-beff-4cc4-e0eb-718fdf07fa62"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'computr' is in the vocabulary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking a word not seen during training (Out-of-Vocabulary)\n",
        "oov_vector = model_ft.wv['computr']  # Misspelled 'computer'\n",
        "print(f\"Embedding for OOV word 'computr':\\n{oov_vector}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSkp2AatwB_Z",
        "outputId": "c7eca138-1c31-453f-a5a6-7dfa9affedb7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for OOV word 'computr':\n",
            "[ 1.2959763e-03  8.6262973e-04  1.0625147e-03 -1.0403705e-04\n",
            " -1.5368442e-03 -1.6962028e-03 -1.8042364e-04 -7.4284733e-04\n",
            "  1.1159903e-03 -1.5890810e-03  2.0372857e-04 -4.8860029e-04\n",
            " -8.2608103e-04 -1.1068498e-03  9.5997169e-04 -4.6363252e-04\n",
            " -7.4049569e-04 -3.5551208e-04 -1.7914166e-03 -4.1690480e-05\n",
            "  6.1781757e-04  2.3780437e-04  1.4869868e-03  2.3090348e-03\n",
            " -7.2294025e-04  9.5657702e-04 -9.6319732e-04 -1.1511220e-03\n",
            "  2.0535337e-04 -8.2215574e-04 -5.9127103e-04 -8.6116942e-04\n",
            " -8.1335296e-05 -1.8366714e-03  8.6622278e-04 -5.4118922e-05\n",
            " -1.3347534e-03 -4.3000176e-04 -1.9358672e-03  2.1117524e-04\n",
            "  1.0365512e-03 -8.5818191e-04  1.8216838e-03 -2.0865335e-03\n",
            " -9.4473729e-04  5.9314049e-04  8.9802052e-04 -2.1881014e-03\n",
            " -5.0291722e-04  8.2109409e-04 -7.3110219e-04  5.9765473e-04\n",
            "  2.4574695e-04 -1.1383955e-03 -3.9746417e-04  1.0086796e-04\n",
            " -1.1171998e-03 -8.2829612e-04  8.1999600e-04 -1.4038280e-03\n",
            "  3.9927711e-04 -3.2045655e-03  2.2712503e-04 -1.2638000e-03\n",
            "  5.8175426e-04 -1.9066310e-03 -2.4197742e-03  6.2398432e-04\n",
            "  1.6639977e-03 -5.1829003e-04  1.4409189e-03  4.4027597e-04\n",
            " -2.9318107e-04 -2.0375075e-03 -3.9017294e-04  6.5125764e-04\n",
            " -7.3168596e-04 -1.5284556e-03 -1.9930236e-03 -6.7336851e-04\n",
            " -2.7116062e-04  5.8327377e-04 -5.0242076e-04 -6.3851185e-04\n",
            "  6.7597465e-04  1.3573465e-03  3.7004554e-04 -1.6667091e-03\n",
            "  1.3504185e-03 -7.7349285e-04  8.4542070e-04  5.9845956e-04\n",
            " -1.0466804e-03 -1.5207485e-03  1.3181550e-04  1.5780111e-03\n",
            "  9.0350606e-04 -5.1118218e-04 -1.9345810e-03 -3.7272290e-05]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference and analysis of above exercise"
      ],
      "metadata": {
        "id": "EuUWu8AS5F30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference from the above exercise**\n",
        "\n",
        "Here we see that the model is incorrectly returning that the misspelled word 'computr' is present in the vocabulary\n",
        "\n",
        "Here's why this happens:\n",
        "\n",
        "The FastText model's word vectors (model_ft.wv) include subword vectors. When you query a word like \"computr\", FastText looks at its subwords and checks whether it can generate a vector by combining the n-grams that exist in the vocabulary.\n",
        "\n",
        "So, when you check for 'computr' in model_ft.wv, FastText may return True because it implicitly recognizes the word based on the subwords that exist in the model, even though the full word \"computr\" itself was never trained directly.\n",
        "\n",
        "FastText Can Dynamically Create Vectors for New Words:\n",
        "\n",
        "Even though \"computr\" wasn't seen in training, FastText can dynamically create a vector for it by using the subword vectors it learned during training. As a result, FastText models can generate vectors for OOV words (out-of-vocabulary words) because they rely on the subword structure.\n",
        "\n",
        "**How FastText Handles Misspelled Words:**\n",
        "\n",
        "Breaking Words into Subwords:\n",
        "\n",
        "FastText represents words as a combination of n-grams (subword units). For example, the word \"computer\" might be broken down into subwords like com, omp, put, uter, etc. These subword embeddings are learned during training.\n",
        "Misspelled Words:\n",
        "\n",
        "When FastText encounters a misspelled word (e.g., \"computr\" instead of \"computer\"), it still tries to break the word into subwords like com, om, put, and utr. Since some subwords of \"computr\" are similar to subwords in \"computer\" (e.g., com and put), the resulting vector for \"computr\" will be close to \"computer\" but not identical.\n",
        "Creating a Vector:\n",
        "\n",
        "FastText generates a new vector for the misspelled word by summing the embeddings of its subwords. Since many subwords overlap between the misspelled and correctly spelled word, the vectors will be similar.\n",
        "Similarity to Correct Word:\n",
        "\n",
        "The vector for the misspelled word (e.g., \"computr\") will be similar to but distinct from the vector for the correct word (\"computer\"). This makes FastText resilient to minor spelling mistakes or variations."
      ],
      "metadata": {
        "id": "q8iSXHA9x7uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important**\n",
        "\n",
        "FastText can generate embeddings for misspelled or OOV words using subwords, but it does not modify its vocabulary or add those words to it.\n",
        "\n",
        "The generated vectors for OOV words are temporary and not retained in the model's vocabulary for future use.\n",
        "\n",
        "This allows FastText to handle spelling variations and OOV words flexibly, but the underlying vocabulary remains static after training."
      ],
      "metadata": {
        "id": "TihiTsvl42gZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To handle the above incorrect response we can use the following code,\n",
        "\n",
        "**model_ft.wv.key_to_index**: This attribute contains the actual words in the vocabulary that were seen during training.\n",
        "\n",
        "By using key_to_index, you are ensuring that only words explicitly present in the training set are counted as being in the vocabulary."
      ],
      "metadata": {
        "id": "6uFwmscT2MGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling OOV word 'computr'\n",
        "if 'computr' in model_ft.wv.key_to_index:\n",
        "    print(f\"'computr' is in the vocabulary\")\n",
        "else:\n",
        "    print(f\"'computr' is not in the vocabulary but FastText can still generate a vector\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga1pHpMW2Kl1",
        "outputId": "4172ae43-8c05-45fc-d67b-01af44685b69"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'computr' is not in the vocabulary but FastText can still generate a vector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus now we get the correct response for the misspelled words"
      ],
      "metadata": {
        "id": "8QZ_7-WF2tNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Fast text with Word2Vec for OOV words"
      ],
      "metadata": {
        "id": "ddQS5nPCyoYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling OOV word 'computr' in Fast Text model\n",
        "if 'computr' in model_ft.wv:\n",
        "    print(f\"'computr' is in the vocabulary\")\n",
        "else:\n",
        "    print(f\"'computr' is not in the vocabulary but FastText can still generate a vector\")\n",
        "\n",
        "# Get the vector for the OOV word\n",
        "oov_word_vector = model_ft.wv['computr']\n",
        "print(f\"OOV vector for 'computr':\\n{oov_word_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0OODG-7zlvs",
        "outputId": "3d9017d7-d53c-496e-cfb5-d13529611848"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'computr' is in the vocabulary\n",
            "OOV vector for 'computr':\n",
            "[ 1.2959763e-03  8.6262973e-04  1.0625147e-03 -1.0403705e-04\n",
            " -1.5368442e-03 -1.6962028e-03 -1.8042364e-04 -7.4284733e-04\n",
            "  1.1159903e-03 -1.5890810e-03  2.0372857e-04 -4.8860029e-04\n",
            " -8.2608103e-04 -1.1068498e-03  9.5997169e-04 -4.6363252e-04\n",
            " -7.4049569e-04 -3.5551208e-04 -1.7914166e-03 -4.1690480e-05\n",
            "  6.1781757e-04  2.3780437e-04  1.4869868e-03  2.3090348e-03\n",
            " -7.2294025e-04  9.5657702e-04 -9.6319732e-04 -1.1511220e-03\n",
            "  2.0535337e-04 -8.2215574e-04 -5.9127103e-04 -8.6116942e-04\n",
            " -8.1335296e-05 -1.8366714e-03  8.6622278e-04 -5.4118922e-05\n",
            " -1.3347534e-03 -4.3000176e-04 -1.9358672e-03  2.1117524e-04\n",
            "  1.0365512e-03 -8.5818191e-04  1.8216838e-03 -2.0865335e-03\n",
            " -9.4473729e-04  5.9314049e-04  8.9802052e-04 -2.1881014e-03\n",
            " -5.0291722e-04  8.2109409e-04 -7.3110219e-04  5.9765473e-04\n",
            "  2.4574695e-04 -1.1383955e-03 -3.9746417e-04  1.0086796e-04\n",
            " -1.1171998e-03 -8.2829612e-04  8.1999600e-04 -1.4038280e-03\n",
            "  3.9927711e-04 -3.2045655e-03  2.2712503e-04 -1.2638000e-03\n",
            "  5.8175426e-04 -1.9066310e-03 -2.4197742e-03  6.2398432e-04\n",
            "  1.6639977e-03 -5.1829003e-04  1.4409189e-03  4.4027597e-04\n",
            " -2.9318107e-04 -2.0375075e-03 -3.9017294e-04  6.5125764e-04\n",
            " -7.3168596e-04 -1.5284556e-03 -1.9930236e-03 -6.7336851e-04\n",
            " -2.7116062e-04  5.8327377e-04 -5.0242076e-04 -6.3851185e-04\n",
            "  6.7597465e-04  1.3573465e-03  3.7004554e-04 -1.6667091e-03\n",
            "  1.3504185e-03 -7.7349285e-04  8.4542070e-04  5.9845956e-04\n",
            " -1.0466804e-03 -1.5207485e-03  1.3181550e-04  1.5780111e-03\n",
            "  9.0350606e-04 -5.1118218e-04 -1.9345810e-03 -3.7272290e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling OOV word 'computr' in Word2Vec model\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec model\n",
        "model_w2v = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, epochs=10)\n",
        "\n",
        "# Try getting a vector for an OOV word in Word2Vec\n",
        "try:\n",
        "    w2v_vector = model_w2v.wv['computr']\n",
        "except KeyError:\n",
        "    print(\"'computr' not found in Word2Vec vocabulary!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxJS94Jeyxeg",
        "outputId": "81e60153-892f-4375-80aa-eb111792adb0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'computr' not found in Word2Vec vocabulary!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Fast text with GloVe for OOV words"
      ],
      "metadata": {
        "id": "75fgB4iEB2Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FastText vs GloVe: Handling OOV Words and Vocabulary Management\n",
        "\n",
        "| **Aspect**                 | **FastText**                                                                                   | **GloVe**                                                                                     |\n",
        "|----------------------------|------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|\n",
        "| **Vocabulary Handling**     | Vocabulary is based on words seen during training, but **subword n-grams** (subword pieces) are learned and used to generate embeddings for OOV words dynamically. | Vocabulary is static and contains only words seen during training. GloVe does **not handle subwords** or dynamically create vectors for OOV words.|\n",
        "| **Handling OOV Words**      | Can handle OOV words by generating embeddings using subword n-grams (e.g., \"computr\" can be broken down into subword pieces like \"com\", \"put\", \"utr\", and generate a vector). | GloVe **cannot handle OOV words**. If you query an OOV word (e.g., \"computr\"), it will result in an error, as no vector exists for the word.|\n",
        "| **Misspelled Words**        | Can generate vectors for misspelled words using subword components, resulting in a vector close to the correct word but not identical. | Treats misspelled words as completely new words, and no vector is generated unless they were part of the training data. |\n",
        "| **Dynamic Vector Creation** | Vectors for OOV and misspelled words are generated dynamically at query time using subwords, but the misspelled word is **not added to the vocabulary**. | GloVe uses static pre-trained vectors, and no new vectors are created after training. No dynamic vector creation for OOV words. |\n",
        "| **Vocabulary Expansion**    | Does not expand the vocabulary when encountering OOV words. It dynamically creates vectors using subword n-grams but does not add the words to the model's vocabulary. | Vocabulary is fixed after training. There is **no capability to expand the vocabulary** or create vectors for OOV words without retraining the model. |\n",
        "| **Vector Consistency**      | Subword modeling helps ensure that similar words (including misspelled words) have similar vectors, reducing the impact of minor variations in word forms. | Vectors are only available for words seen during training. Misspelled or new words have no vector, leading to consistency issues with OOV words. |\n",
        "| **Training Data**           | Works well even on smaller datasets because it uses subword information to generalize to unseen words. | Requires large datasets for good performance because the co-occurrence statistics rely on seeing enough examples of each word. |\n",
        "| **Pre-trained Models**      | Pre-trained FastText models are available and can be used for various tasks, though they can be further fine-tuned on new data. | Pre-trained GloVe models are widely available and are typically used as-is since GloVe cannot be dynamically fine-tuned easily without retraining. |\n",
        "\n",
        "---\n",
        "\n",
        "### Key Differences Between FastText and GloVe:\n",
        "\n",
        "1. **Subword Information**:\n",
        "   - **FastText**: Leverages subwords (character n-grams), which allows it to generate vectors for unseen or misspelled words.\n",
        "   - **GloVe**: Does not use subwords, which means it cannot handle OOV words and requires retraining to expand vocabulary.\n",
        "\n",
        "2. **OOV Word Handling**:\n",
        "   - **FastText**: Can handle OOV words dynamically using subwords. This makes it much more robust to spelling variations, new words, or morphological changes.\n",
        "   - **GloVe**: Fails with OOV words. If a word wasnâ€™t seen during training, no vector is generated, and the model will return an error.\n",
        "\n",
        "3. **Vocabulary Expansion**:\n",
        "   - **FastText**: Does not expand its vocabulary after training, but it dynamically generates vectors using subwords.\n",
        "   - **GloVe**: No vocabulary expansion is possible unless the model is retrained.\n",
        "\n",
        "4. **Efficiency in Handling Misspellings**:\n",
        "   - **FastText**: Handles minor variations and misspellings well because it can still create vectors based on subwords.\n",
        "   - **GloVe**: Treats misspellings as entirely new words and fails to generate a vector unless explicitly trained on those variations.\n",
        "\n"
      ],
      "metadata": {
        "id": "-L5eVEWzBVzv"
      }
    }
  ]
}