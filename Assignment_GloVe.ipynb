{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swap1984/swapnil/blob/main/Assignment_GloVe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMNhuBza4ton"
      },
      "source": [
        "#**Glove Embedding method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvG6d7_Z44MQ"
      },
      "source": [
        "**What Are GloVe Embeddings?**\n",
        "\n",
        "GloVe (Global Vectors for Word Representation) is a **popular pre-trained word embedding technique developed by Stanford.** It is based on the idea that the meaning of a word is captured by the co-occurrence probability of that word with other words in a large corpus. **GloVe embeddings are trained on a global word co-occurrence matrix, which gives them the ability to capture both the local context and global semantics.**\n",
        "\n",
        "**Advantages of GloVe Embeddings:**\n",
        "\n",
        "Efficient Pre-trained Embeddings:\n",
        "You can load pre-trained embeddings to save time, rather than training from scratch.\n",
        "\n",
        "Captures Word Meaning Well:\n",
        "GloVe embeddings capture semantic relationships, e.g., the vector for \"king\" – \"man\" + \"woman\" is close to \"queen.\"\n",
        "\n",
        "Available in Multiple Sizes:\n",
        "Pre-trained models are available with different dimensions (e.g., 50D, 100D, 300D), allowing you to choose based on your requirements.\n",
        "\n",
        "Handles Context:\n",
        "While not as powerful as contextual embeddings (like BERT), GloVe captures more global context than simple word embeddings like Word2Vec.\n",
        "\n",
        "**Disadvantages of GloVe Embeddings:**\n",
        "\n",
        "Static Embeddings:\n",
        "GloVe assigns the same vector to a word, irrespective of the context. For example, the word \"bank\" will have the same vector whether referring to a financial institution or a riverbank.\n",
        "\n",
        "Pre-trained on Specific Corpora:\n",
        " The pre-trained GloVe embeddings are trained on certain corpora (e.g., Wikipedia, Common Crawl), which may not always match your dataset's vocabulary.\n",
        "\n",
        "Cannot Handle Out-of-Vocabulary (OOV) Words:\n",
        "Words that were not present in the training data are not included in the pre-trained GloVe embeddings, making them harder to handle for domain-specific text.\n",
        "\n",
        "**Applications of GloVe Embeddings:**\n",
        "\n",
        "Text Classification:\n",
        "Word embeddings from GloVe can be used as input features for text classifiers.\n",
        "\n",
        "Semantic Search:\n",
        "Use embeddings to compute similarity between queries and documents.\n",
        "\n",
        "Named Entity Recognition (NER):\n",
        " Word embeddings help in recognizing entities by capturing semantic relationships between words.\n",
        "\n",
        "Sentiment Analysis:\n",
        " Pre-trained embeddings improve the performance of sentiment classifiers by capturing meaning.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Qb7Po_8zyb"
      },
      "source": [
        "# **GloVe Embeddings Pretrained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E9SkRmHk36YS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b2a913-7737-4123-a9e8-2a29e54037c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# initialising the libraries\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize  # Ensure you have NLTK installed\n",
        "import nltk\n",
        "nltk.download('punkt')  # Download the punkt tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4hnZ4zPH_FDY"
      },
      "outputs": [],
      "source": [
        "# using a paragraph as input . data =\"\"   \"\"\n",
        "data = \"\"\"Yes, life is full, there is life even underground,” he began again. “You wouldn’t believe, Alexey, how I want to live now, what a thirst for existence and consciousness has sprung up in me within these peeling walls… And what is suffering? I am not afraid of it, even if it were beyond reckoning. I am not afraid of it now. I was afraid of it before… And I seem to have such strength in me now, that I think I could stand anything, any suffering, only to be able to say and to repeat to myself every moment, ‘I exist.’ In thousands of agonies — I exist. I’m tormented on the rack — but I exist! Though I sit alone on a pillar — I exist! I see the sun, and if I don’t see the sun, I know it’s there. And there’s a whole life in that, in knowing that the sun is there.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp2w4VMCBZ3E",
        "outputId": "055fae0e-6d9d-4837-9903-41411e6b1b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-01 17:13:00--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-10-01 17:13:00--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-10-01 17:13:01--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  4.87MB/s    in 2m 39s  \n",
            "\n",
            "2024-10-01 17:15:40 (5.16 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the GloVe 6B vectors (around 812Mb)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mluabYT9C6Xz",
        "outputId": "f916d740-e45e-4d1f-c96c-b8f5ce90260a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip.1\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "# Unzip the file\n",
        "!unzip glove.6B.zip.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kM3rabTR9IXO"
      },
      "outputs": [],
      "source": [
        "# Define the file path for GloVe pre-trained embeddings\n",
        "glove_file = r'/content/glove.6B.200d.txt'  # Example for 200-dimensional embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(glove_file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == \"\":\n",
        "                continue\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            try:\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = coefs\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return embeddings_index\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jok-rAUsKPmK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert tokens to embeddings\n",
        "def text_to_embeddings(tokens, glove_embeddings):\n",
        "    embeddings = []\n",
        "    for word in tokens:\n",
        "        if word in glove_embeddings:\n",
        "            embeddings.append(glove_embeddings[word])\n",
        "        else:\n",
        "            # Append a zero vector of the appropriate dimension if the word is not found\n",
        "            embeddings.append(np.zeros((200,)))  # Adjust if using different dimensions\n",
        "    return np.array(embeddings,dtype='float32')"
      ],
      "metadata": {
        "id": "M3JyDhEgOACZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "glove_file_path = '/content/glove.6B.200d.txt'  # Replace with the path to your GloVe file\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path)"
      ],
      "metadata": {
        "id": "idu0rNTUKfdE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input data\n",
        "tokens = word_tokenize(data.lower())  # Tokenize and convert to lowercase\n",
        "print(f\"Tokens: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjPv2-5WL48F",
        "outputId": "00f8adbc-4eb3-441e-862f-931ac98d08b8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['yes', ',', 'life', 'is', 'full', ',', 'there', 'is', 'life', 'even', 'underground', ',', '”', 'he', 'began', 'again', '.', '“', 'you', 'wouldn', '’', 't', 'believe', ',', 'alexey', ',', 'how', 'i', 'want', 'to', 'live', 'now', ',', 'what', 'a', 'thirst', 'for', 'existence', 'and', 'consciousness', 'has', 'sprung', 'up', 'in', 'me', 'within', 'these', 'peeling', 'walls…', 'and', 'what', 'is', 'suffering', '?', 'i', 'am', 'not', 'afraid', 'of', 'it', ',', 'even', 'if', 'it', 'were', 'beyond', 'reckoning', '.', 'i', 'am', 'not', 'afraid', 'of', 'it', 'now', '.', 'i', 'was', 'afraid', 'of', 'it', 'before…', 'and', 'i', 'seem', 'to', 'have', 'such', 'strength', 'in', 'me', 'now', ',', 'that', 'i', 'think', 'i', 'could', 'stand', 'anything', ',', 'any', 'suffering', ',', 'only', 'to', 'be', 'able', 'to', 'say', 'and', 'to', 'repeat', 'to', 'myself', 'every', 'moment', ',', '‘', 'i', 'exist.', '’', 'in', 'thousands', 'of', 'agonies', '—', 'i', 'exist', '.', 'i', '’', 'm', 'tormented', 'on', 'the', 'rack', '—', 'but', 'i', 'exist', '!', 'though', 'i', 'sit', 'alone', 'on', 'a', 'pillar', '—', 'i', 'exist', '!', 'i', 'see', 'the', 'sun', ',', 'and', 'if', 'i', 'don', '’', 't', 'see', 'the', 'sun', ',', 'i', 'know', 'it', '’', 's', 'there', '.', 'and', 'there', '’', 's', 'a', 'whole', 'life', 'in', 'that', ',', 'in', 'knowing', 'that', 'the', 'sun', 'is', 'there', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create the Embedding Matrix\n",
        "def get_embedding_matrix(word_index, glove_embeddings, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # +1 for padding or unknowns\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = glove_embeddings.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n"
      ],
      "metadata": {
        "id": "LZCMtNgaanDa"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens to embeddings\n",
        "embeddings_array = text_to_embeddings(tokens, glove_embeddings)"
      ],
      "metadata": {
        "id": "Oiil5IlAOQjy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the embeddings\n",
        "print(f\"Shape of the embeddings array: {embeddings_array.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOtRB9OhS7nB",
        "outputId": "3f71661f-b5d8-494b-aa4a-77b2787e877e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the embeddings array: (193, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the first 5 embeddings for the first 5 tokens of the data\n",
        "print(embeddings_array[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFw8UXBZTB4w",
        "outputId": "e57b9c50-1282-4d52-f72d-d827097d2d10"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.2042e-01  2.4924e-01  1.7393e-01 -1.6275e-02 -3.0916e-01  2.6610e-01\n",
            "  -7.4482e-01  8.3853e-02  3.2016e-01  6.5026e-01 -3.0558e-01  1.6842e-02\n",
            "   4.8508e-02 -2.9494e-01 -7.8175e-01  4.1424e-01 -2.1490e-01  5.3132e-01\n",
            "   4.8179e-01  1.5397e-01  3.0971e-01  1.4896e+00 -8.2240e-02  3.2086e-01\n",
            "   2.8866e-02 -2.9613e-01 -1.3558e-01 -7.4538e-02  2.3826e-01 -5.3784e-01\n",
            "  -4.1572e-01 -1.3477e-01  1.5444e-01 -1.2929e-01 -3.4317e-01 -1.5537e-01\n",
            "   3.3685e-02 -3.5089e-01 -2.2403e-01  4.9218e-01 -1.4750e-01  3.4514e-03\n",
            "  -2.4207e-01 -1.0924e-01  5.7027e-03  1.2135e-01  6.0403e-01 -1.7868e-01\n",
            "  -3.9604e-01  5.6209e-02  5.1824e-01 -3.9824e-01  1.3078e-01  1.3691e-01\n",
            "  -1.5311e-01 -4.9687e-02  4.9536e-02 -3.3107e-02 -1.2196e-02 -3.2973e-01\n",
            "   2.5021e-01 -3.2097e-01  1.4448e-01  5.5550e-02  6.1817e-02 -7.5844e-02\n",
            "  -1.9434e-01  6.2882e-01  8.3724e-01  1.1561e-01  1.9471e-01  3.9750e-03\n",
            "   1.7576e-02 -3.6561e-01 -4.1066e-01 -4.7927e-01 -5.2158e-01  2.1736e-01\n",
            "  -5.6578e-01 -1.5558e-01  3.5471e-01 -7.0188e-03 -1.9452e-03  1.7939e-01\n",
            "  -3.5319e-01 -2.5795e-01 -3.1974e-01 -4.4591e-02  1.2496e-01 -9.3906e-01\n",
            "  -1.4558e-01 -2.3784e-01  3.3792e-01 -2.5189e-01 -2.2839e-01  7.8948e-02\n",
            "   3.5851e-01  2.1203e-02 -8.1422e-01  2.3170e-01  4.7906e-01  9.6946e-02\n",
            "   2.3977e-01 -3.8182e-01  3.7931e-01 -3.6341e-02 -3.6294e-01  1.3091e+00\n",
            "  -4.8050e-02  3.7740e-01  1.4168e-02  1.2427e-01  8.0578e-02  2.1790e-02\n",
            "  -5.7981e-01  1.9524e-02 -8.1258e-02 -2.1899e-02 -2.8081e-01  2.1055e-01\n",
            "   1.9828e-01 -8.3682e-02  1.4192e-01  6.8194e-02  2.0974e-01 -4.5658e-01\n",
            "  -2.3901e-02 -6.1730e-01  4.8950e-01 -3.0713e-01 -1.5431e-01 -1.9995e-01\n",
            "  -3.6085e-02  1.7406e-01 -7.2787e-03 -1.0505e-01 -1.4231e-01 -2.1916e-01\n",
            "   1.7860e-01 -2.7257e-02  1.0534e-01  7.3802e-02  1.2930e-01 -4.1754e-01\n",
            "   1.2693e+00  1.4427e-01  3.7295e-01 -4.2493e-01 -6.9197e-01  8.0780e-02\n",
            "   1.4119e-02  6.7835e-01  1.6693e-01  2.6872e-01  3.2128e-01  4.6269e-03\n",
            "  -2.1230e-02 -1.3293e-01  1.4867e-01  3.6675e-02 -9.8082e-02 -2.4252e-01\n",
            "  -5.5073e-02 -3.3584e-01 -3.2442e-01 -5.4760e-01  2.1091e-01  1.2983e-01\n",
            "  -1.4290e-01 -6.0260e-01 -7.6548e-02  4.5740e-01  2.7267e-01  1.2926e-01\n",
            "  -2.1670e-01 -1.5686e-01 -4.6502e-01  1.5973e-01 -1.5449e-01  3.6390e-01\n",
            "   9.7274e-01  1.1432e-01 -3.3772e-02  2.3939e-02 -2.3805e-01 -3.1564e-01\n",
            "   5.0120e-01  4.9548e-02  2.0645e-01 -6.6727e-01 -2.3230e-01  1.1915e-01\n",
            "   2.2961e-01 -4.1109e-01  5.0235e-02 -3.8869e-01 -9.6837e-02 -8.8502e-02\n",
            "  -1.9784e-01  3.8890e-01]\n",
            " [ 1.7651e-01  2.9208e-01 -2.0768e-03 -3.7523e-01  4.9139e-03  2.3979e-01\n",
            "  -2.8893e-01 -1.4643e-02 -1.0993e-01  1.5592e-01  2.0627e-01  4.7675e-01\n",
            "   9.9907e-02 -1.4058e-01  2.1114e-01  1.2126e-01 -3.1831e-01 -8.9433e-02\n",
            "  -9.0553e-02 -3.1962e-01  2.1319e-01  2.4844e+00 -7.7521e-02 -8.4279e-02\n",
            "   2.0186e-01  2.6084e-01 -4.0411e-01 -1.9127e-01  2.4715e-01  2.2394e-01\n",
            "  -6.3437e-02  2.0379e-01 -1.8463e-01 -8.8413e-02  2.4169e-02 -2.8769e-01\n",
            "  -6.1246e-01 -1.2683e-01 -8.8273e-02  1.8331e-01 -5.3161e-01 -1.9970e-01\n",
            "  -2.6703e-01  1.5312e-01 -1.5239e-02 -8.2844e-02  4.7856e-01 -2.9612e-01\n",
            "   1.1168e-01 -2.5790e-02 -1.1697e-02  1.9923e-01 -1.4267e-01  6.6250e-01\n",
            "  -5.1739e-02 -1.6938e-01 -1.5635e-01  9.2806e-02  3.2548e-01  1.1724e-01\n",
            "   2.8788e-01 -6.0651e-02 -1.4153e-01  1.6668e-01  2.6861e-01 -3.1001e-02\n",
            "  -3.9665e-01  3.5304e-01  2.3850e-01  1.2388e-01  4.5698e-01 -1.2559e-01\n",
            "  -1.2804e-01  3.7449e-01  2.4460e-01  2.3073e-01  2.0808e-01  5.1258e-02\n",
            "  -2.1816e-01 -3.6409e-02 -3.8800e-02 -4.2487e-02 -3.0779e-01 -2.5449e-02\n",
            "   2.2532e-01  4.5538e-02 -4.8934e-01 -1.3988e-01  1.7394e-01 -4.6137e-01\n",
            "  -2.6555e-01  1.5473e-01  6.3816e-02 -1.7022e-01 -1.5762e-01  7.5765e-02\n",
            "   1.2151e-01 -4.9340e-01 -1.0909e-01  3.4487e-02  2.9947e-01  1.8690e-02\n",
            "  -1.6534e-01  1.6679e-02  1.6341e-01 -2.7418e-01  7.7797e-02  1.4023e+00\n",
            "   2.5275e-02  9.4725e-02 -4.0735e-02 -1.0642e-01  2.3364e-02  7.9143e-02\n",
            "  -1.6615e-01 -2.3013e-01 -1.4071e-01  4.0159e-01 -3.4951e-01  1.8545e-02\n",
            "   2.2434e-01  7.6922e-01  2.4722e-01  1.4936e-01  4.2368e-01 -7.2059e-01\n",
            "  -3.8541e-02  1.5522e-01  3.3596e-01 -4.3077e-01 -2.6925e-02 -3.7733e-01\n",
            "   2.4271e-01 -4.6495e-01  4.5783e-01  2.3693e-01  7.9361e-02 -3.2244e-01\n",
            "  -4.2434e-01 -1.1138e-01  5.5426e-01  8.5153e-02 -2.0581e-02 -4.6386e-02\n",
            "   1.2467e+00  1.3177e-01  6.7092e-02 -5.7780e-01  1.3586e-02 -7.1274e-02\n",
            "   1.7311e-02  8.9781e-02  1.9857e-01 -3.2205e-02  6.4843e-01 -2.3797e-01\n",
            "  -1.9676e-01  2.0203e-01  2.1074e-01 -5.0347e-01  2.6823e-02 -4.5444e-02\n",
            "  -2.2642e-01 -1.9977e-01 -1.2138e-01  1.6941e-01  6.1998e-02  4.2631e-01\n",
            "  -8.8383e-02  4.5756e-01  7.7774e-02  6.1342e-02  4.5710e-01 -1.7787e-01\n",
            "  -1.4597e-01  3.2654e-01  2.4430e-03 -1.1886e-01  1.0081e-01 -2.0011e-02\n",
            "   1.0366e+00 -3.9814e-01 -6.8180e-01  2.3685e-01 -2.0396e-01 -1.7668e-01\n",
            "  -3.1385e-01  1.4834e-01 -5.2187e-02  6.1300e-02 -3.2582e-01  1.9153e-01\n",
            "  -1.5469e-01 -1.4679e-01  4.6971e-02  3.2325e-02 -2.2006e-01 -2.0774e-01\n",
            "  -2.3189e-01 -1.0814e-01]\n",
            " [ 3.4098e-01  4.1888e-01 -3.1878e-01  3.1399e-02  4.7223e-02  2.4906e-01\n",
            "  -7.1631e-01  5.2904e-02 -3.4208e-03  2.1647e-01 -3.0463e-02  8.9968e-01\n",
            "   3.5829e-01 -6.6277e-02  8.1989e-02  3.9360e-01  9.9335e-02  7.3747e-01\n",
            "   3.2178e-01 -2.2824e-01  3.9632e-01  2.9324e+00  2.0929e-01 -2.6525e-01\n",
            "   1.2720e-01  8.7867e-02 -3.5642e-03 -5.1395e-02 -3.8543e-01  2.6079e-01\n",
            "   5.8987e-02 -3.2565e-02  3.4674e-01 -6.5351e-02 -1.2305e-01  1.8915e-02\n",
            "  -1.9741e-01  2.4483e-01 -7.6178e-02  1.4100e-01 -9.6150e-01 -1.7128e-02\n",
            "  -1.9702e-01  5.8651e-01 -1.0661e-01  1.0472e-01  6.7752e-01  4.9478e-01\n",
            "   1.1170e-01  1.8718e-02 -2.6009e-01  4.9720e-01 -1.5728e-01  6.0994e-01\n",
            "   1.8953e-01  1.1996e-01  2.7827e-01 -1.5831e-02  3.9711e-02 -3.7136e-01\n",
            "  -2.8909e-01 -3.2909e-01 -7.1628e-01  5.6670e-01 -2.3132e-01 -1.4021e-01\n",
            "   5.5793e-01  3.1357e-01 -5.7165e-01  3.4597e-01  7.1454e-01 -1.7147e-01\n",
            "   8.9315e-02  8.1979e-01  2.3206e-01  1.9429e-01 -4.4881e-01 -2.8906e-01\n",
            "  -1.5707e-02  1.4476e-01 -1.4431e-02  1.1904e-01 -6.6498e-01 -3.8981e-02\n",
            "   1.7994e-01  9.7028e-02 -3.4582e-01  6.7696e-03  4.7810e-01 -6.0951e-01\n",
            "   6.4572e-01  7.3228e-04  3.7128e-01  1.7062e-01 -2.6385e-01 -4.2804e-01\n",
            "  -2.2658e-01 -1.8425e-01  5.2100e-03  1.8815e-01  1.2560e-01  6.0294e-02\n",
            "  -2.0199e-01  2.6285e-02  2.4987e-02 -7.4668e-01  2.0404e-01  7.0431e-01\n",
            "  -2.0175e-01 -2.1715e-01  1.6100e-01 -1.3599e-01  1.9623e-01  2.2066e-02\n",
            "  -3.3890e-01 -1.9406e-01 -1.5501e-01  5.4023e-01 -6.1807e-01 -3.9910e-01\n",
            "  -1.7745e-01  2.6452e-01 -1.2936e-01 -1.9009e-01 -8.3096e-02 -3.2137e-01\n",
            "   5.8294e-01  1.6789e-01  4.7591e-02 -9.2068e-01 -3.5928e-01 -1.0102e-01\n",
            "   9.4616e-02  5.1867e-02  8.9359e-02  1.6542e-01  7.5411e-01 -1.1097e-01\n",
            "  -2.3721e-01 -1.3093e-01  5.3672e-01  3.0311e-01 -8.0717e-02 -1.6100e-01\n",
            "   1.4385e+00  1.9252e-02 -1.5350e-01  1.8693e-02  2.7981e-01  4.8011e-01\n",
            "   7.4099e-02 -1.0303e-01 -2.4328e-01  6.9507e-02  3.4220e-01 -1.9133e-01\n",
            "   5.9109e-02 -2.8913e-02 -1.6200e-01  3.7345e-01  1.4532e-01  7.4352e-01\n",
            "   9.0516e-02 -3.8116e-01  1.1042e-01 -2.5854e-01 -4.0525e-01 -3.6723e-02\n",
            "  -1.8737e-01  3.2509e-01 -2.7669e-01  3.1600e-01  3.0019e-01  5.3721e-01\n",
            "  -4.3686e-01  1.5107e-01 -2.3496e-01 -9.7297e-02  4.1126e-01  6.4296e-01\n",
            "   1.2471e+00 -1.7064e-01 -3.7883e-01  3.1271e-02 -1.9095e-01  5.0932e-01\n",
            "   4.7580e-02 -9.3402e-02  6.1584e-01  2.1330e-01 -2.4400e-01  5.7573e-02\n",
            "  -1.7811e-01 -1.7605e-01  1.4003e-01 -6.9920e-01  2.8465e-01  7.2919e-01\n",
            "   3.8914e-01  3.7687e-01]\n",
            " [ 3.2928e-01  2.5526e-01  2.6753e-01 -8.4809e-02  2.9764e-01  6.2339e-02\n",
            "  -1.5475e-01  1.7784e-01  3.2328e-01 -9.2752e-01  1.5194e-01  1.6324e-01\n",
            "  -1.0428e-01 -2.6464e-02  6.5971e-01  1.4782e-01  3.8623e-01  2.5169e-01\n",
            "   1.2610e-01 -4.3138e-01  2.8092e-01  3.1604e+00 -1.7565e-01 -3.2247e-03\n",
            "   6.4389e-01 -3.9697e-01  1.8975e-01  3.7999e-01 -7.9175e-02 -1.4781e-01\n",
            "  -7.2965e-02  5.7247e-02 -4.2314e-01  4.5080e-01 -9.7386e-02 -4.7587e-01\n",
            "  -9.6599e-01 -7.5595e-01 -3.3932e-02 -7.0886e-02 -4.4828e-01 -5.2094e-01\n",
            "  -1.8230e-01  1.8582e-01 -7.4273e-02 -1.7871e-02  1.6742e-01  1.5459e-02\n",
            "   3.0290e-01 -1.2580e-01  3.2418e-01 -3.1263e-01 -7.6832e-02  5.1959e-02\n",
            "   2.7242e-01 -1.8285e-01 -3.6479e-01 -6.3562e-01 -2.1685e-01  3.5812e-02\n",
            "   1.2485e-01  3.7268e-01 -1.6976e-01 -9.4146e-02 -1.6412e-01 -1.0728e-01\n",
            "   3.7866e-02  1.1750e-01 -1.5533e-01  3.4062e-01  5.8848e-01  3.8992e-01\n",
            "  -5.4839e-01  8.5013e-01 -8.3728e-01  1.5482e-01 -3.7191e-01 -6.5409e-01\n",
            "  -2.7631e-01 -2.5224e-02  7.5732e-02 -2.3904e-01 -1.8311e-01 -8.4571e-02\n",
            "   1.5492e-01 -1.6317e-01 -2.6499e-01  5.6831e-02  8.8287e-01 -4.7655e-01\n",
            "   2.5131e-01 -9.3160e-02  3.4377e-01 -3.5863e-01 -2.2855e-01  1.1918e-01\n",
            "   2.9661e-01 -2.5360e-01  4.9002e-02 -2.1234e-01  1.6237e-01  5.3871e-01\n",
            "   3.5344e-02  3.9293e-01 -2.9673e-01 -7.2556e-01 -2.7431e-01  1.3469e+00\n",
            "  -1.9218e-01  5.0534e-01  2.8451e-02 -3.2206e-01  9.6035e-02 -8.3551e-03\n",
            "  -1.3107e-02 -3.2444e-01 -1.0163e-01  3.1755e-02 -6.3196e-01 -2.1541e-01\n",
            "  -3.5609e-02  3.1259e-01  2.3988e-01 -1.9056e-01 -1.3086e-01 -1.2644e-01\n",
            "   4.8795e-01 -1.3492e-01 -4.1967e-01  1.5904e-01 -2.7921e-01 -1.7258e-02\n",
            "   2.9370e-01  6.7436e-02  8.5052e-02  9.9394e-02 -5.5281e-03  9.4985e-02\n",
            "   1.1167e-01  1.9749e-01  2.5230e-01  3.2205e-01  4.2778e-01 -3.5180e-02\n",
            "   1.3291e+00  5.2610e-03  2.6769e-01 -4.6168e-01  1.1250e-01  1.0111e-01\n",
            "  -3.1174e-01  5.4580e-01 -3.7363e-01 -2.6133e-02  9.9566e-01 -1.5827e-01\n",
            "  -2.6202e-01  1.7324e-01  6.0104e-02 -4.8004e-01  2.3841e-01 -2.1495e-01\n",
            "   7.7693e-02 -8.9078e-02  1.2985e-01 -1.7400e-01 -5.7151e-02  4.8207e-01\n",
            "  -1.4668e-01  2.6739e-01 -3.3366e-01  3.2552e-01  6.2520e-01 -3.0905e-01\n",
            "   8.7737e-02 -1.7204e-01  2.8246e-01 -3.7268e-02  1.6007e-01  3.0031e-01\n",
            "   1.4061e+00 -3.2169e-01 -2.5792e-02  3.7175e-02  2.6222e-02 -2.7671e-01\n",
            "   5.1688e-02 -5.8734e-02 -2.3223e-01 -1.0529e-01 -4.0318e-01 -2.2161e-01\n",
            "   6.0587e-02  9.1321e-02 -2.1363e-01  7.1634e-02 -2.1331e-01  7.4621e-02\n",
            "   1.2001e-02 -2.1952e-01]\n",
            " [ 2.9667e-01 -1.9041e-01 -1.6734e-01 -2.3067e-01  3.5460e-02 -1.5565e-01\n",
            "   1.0894e-01 -1.8320e-01 -7.8841e-02  3.1029e-01 -2.6727e-01  2.6272e-01\n",
            "   2.6124e-01 -3.3728e-01  4.2254e-01 -4.8555e-01  1.3919e-01  3.5297e-01\n",
            "  -1.9247e-01 -2.2333e-01  4.9861e-01  2.7357e+00  1.3011e-01 -5.0954e-01\n",
            "   4.5014e-01 -8.1410e-02 -2.3534e-01  1.0371e-02  4.3054e-01 -2.1223e-01\n",
            "  -1.9638e-01 -3.1434e-01 -1.2297e-01 -5.6133e-01 -9.0384e-02 -3.3263e-02\n",
            "  -3.2137e-01 -5.4950e-01 -1.0027e-02  2.3146e-01  1.0421e-02 -1.3659e-02\n",
            "  -7.6713e-03 -1.3176e-01  1.0317e-02  3.7078e-01  4.6923e-01  5.9423e-02\n",
            "   4.1123e-01  1.0023e-02 -9.0452e-02 -4.0759e-01 -1.1723e-01  3.0375e-01\n",
            "   2.5175e-01  1.5379e-01  1.5170e-01 -2.0262e-02 -1.7740e-01 -1.8909e-01\n",
            "   6.7966e-01 -6.6682e-01 -4.9400e-01 -5.9751e-01  1.0683e-01  1.3225e-01\n",
            "   2.7032e-01 -1.0069e-01  2.5423e-01  1.3450e-01  5.0991e-01  2.7086e-01\n",
            "   3.4327e-01  2.3083e-02 -1.2101e-01  1.7861e-01 -1.9951e-01 -2.3792e-01\n",
            "   4.3102e-01  5.5752e-02  1.6033e-01 -3.4647e-01 -2.1565e-01 -7.6750e-02\n",
            "   1.7376e-01 -2.6902e-01  2.2254e-01 -3.2923e-01  7.6815e-01 -7.1194e-01\n",
            "   3.4376e-01  2.2850e-01  4.1817e-01  1.3746e-02  2.8038e-01 -3.1916e-01\n",
            "   3.1060e-01  2.9356e-01 -2.8953e-01  1.0565e-01  2.7560e-01 -3.7460e-01\n",
            "   6.0083e-01  1.7285e-04 -2.1326e-01  3.0623e-02  9.0689e-02  1.0639e+00\n",
            "  -4.1776e-01 -2.2014e-01 -3.7017e-01 -4.8046e-01  6.7010e-02 -7.5316e-02\n",
            "   8.5481e-02 -4.0933e-01  2.6082e-01 -2.5018e-01  9.1720e-02 -1.2541e-01\n",
            "  -4.6252e-01  2.0970e-01 -2.3605e-01  2.8665e-01  3.5655e-02 -3.4872e-01\n",
            "   4.5211e-01 -1.3086e-01  3.5587e-01  6.8709e-02 -4.8611e-02 -7.7557e-03\n",
            "   9.3124e-02 -1.6243e-01  3.5604e-01  2.6036e-01  1.2996e-01 -5.3751e-01\n",
            "   5.2925e-01  1.7803e-01  4.2875e-01 -3.2468e-01  2.8189e-01  3.7518e-01\n",
            "   1.2964e+00  3.4064e-02 -3.0608e-01 -1.6244e-01 -1.8960e-01 -1.9614e-01\n",
            "   3.5166e-01 -2.4670e-01 -2.1728e-01  1.9992e-01  2.7304e-02 -1.4604e-01\n",
            "  -3.4660e-01 -1.2907e-02  6.7934e-02 -3.3142e-01  2.1655e-01 -2.3391e-01\n",
            "   2.1805e-01  4.7136e-02  1.8012e-01  1.2672e-01 -5.3111e-01 -4.8423e-01\n",
            "  -5.1849e-01  6.2180e-01  2.8892e-02  6.2254e-02  2.1884e-01  4.6848e-01\n",
            "  -3.4412e-02  1.1361e-01 -6.7913e-01 -3.8568e-01 -3.0395e-01 -4.9260e-02\n",
            "   1.3075e+00 -5.2156e-01 -3.1272e-01 -2.5363e-01 -1.0971e-01 -2.0867e-01\n",
            "  -4.4347e-01  1.1496e-01  1.4502e-01  3.9719e-01 -1.6284e-01  3.0485e-01\n",
            "  -1.3438e-01 -6.4691e-01  1.1908e-01 -5.3820e-01  5.5905e-01 -4.4982e-02\n",
            "  -5.2480e-01  4.3902e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve the embedding\n",
        "def get_embedding(token, glove_model, embedding_dim=300):\n",
        "    return glove_model.get(token, np.zeros(embedding_dim))\n"
      ],
      "metadata": {
        "id": "m0A-ZJr6Wl2C"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve embeddings for the tokens\n",
        "embeddings_array = [get_embedding(token, glove_embeddings) for token in tokens]"
      ],
      "metadata": {
        "id": "ASSGUqZ_WssX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the output\n",
        "for token, embedding in zip(tokens, embeddings_array):\n",
        "    print(f\"Token: '{token}', Embedding: {embedding[:5]}\")  # Print first 5 dimensions of the embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waJ6kDNeWxhq",
        "outputId": "abb3a3d2-ca4a-4918-c77a-cb96ae569822"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: 'yes', Embedding: [ 0.22042   0.24924   0.17393  -0.016275 -0.30916 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'life', Embedding: [ 0.34098   0.41888  -0.31878   0.031399  0.047223]\n",
            "Token: 'is', Embedding: [ 0.32928   0.25526   0.26753  -0.084809  0.29764 ]\n",
            "Token: 'full', Embedding: [ 0.29667 -0.19041 -0.16734 -0.23067  0.03546]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'there', Embedding: [ 0.66193   0.16192  -0.090129 -0.59287   0.15391 ]\n",
            "Token: 'is', Embedding: [ 0.32928   0.25526   0.26753  -0.084809  0.29764 ]\n",
            "Token: 'life', Embedding: [ 0.34098   0.41888  -0.31878   0.031399  0.047223]\n",
            "Token: 'even', Embedding: [ 0.44802   0.16025  -0.23372  -0.054205 -0.067149]\n",
            "Token: 'underground', Embedding: [-0.20773 -0.26716 -0.64454 -0.14187  0.3224 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: '”', Embedding: [ 0.10706   0.25534   0.036386 -0.01086  -0.1454  ]\n",
            "Token: 'he', Embedding: [ 0.10278  -0.037982 -0.34679  -0.20236  -0.10104 ]\n",
            "Token: 'began', Embedding: [-0.68062   0.057562 -0.48837  -0.6388   -0.39483 ]\n",
            "Token: 'again', Embedding: [-0.016971  0.39541  -0.50494   0.012802 -0.36807 ]\n",
            "Token: '.', Embedding: [ 0.12289   0.58037  -0.069635 -0.50288   0.10503 ]\n",
            "Token: '“', Embedding: [ 0.20132   0.21419  -0.044341  0.057602 -0.19705 ]\n",
            "Token: 'you', Embedding: [ 0.85395   0.57146  -0.023652 -0.11047  -0.1275  ]\n",
            "Token: 'wouldn', Embedding: [-0.11372 -0.25854  0.1247   0.59396  0.39038]\n",
            "Token: '’', Embedding: [ 0.21752  -0.11792  -0.088778  0.33265  -0.23704 ]\n",
            "Token: 't', Embedding: [ 0.42596 -0.18836 -0.65115  0.72988  0.86216]\n",
            "Token: 'believe', Embedding: [-0.095714  0.17591   0.17628  -0.10767  -0.073885]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'alexey', Embedding: [0.54669  0.58661  0.43686  0.097392 0.14128 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'how', Embedding: [ 0.18146   0.2663    0.054955  0.23884  -0.047094]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'want', Embedding: [ 0.43249   0.53471  -0.018324  0.15637   0.066969]\n",
            "Token: 'to', Embedding: [ 0.57346  0.5417  -0.23477 -0.3624   0.4037 ]\n",
            "Token: 'live', Embedding: [ 0.28944  0.13113  0.11904 -0.15172 -0.73016]\n",
            "Token: 'now', Embedding: [ 0.17821  -0.043552 -0.047699  0.20681   0.18358 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'what', Embedding: [ 0.39396    0.44185   -0.0042279 -0.044507  -0.13598  ]\n",
            "Token: 'a', Embedding: [ 0.24169 -0.34534 -0.22307 -1.2907   0.25285]\n",
            "Token: 'thirst', Embedding: [0.57785  0.31791  0.066497 0.18393  0.1795  ]\n",
            "Token: 'for', Embedding: [ 0.29117   0.86093  -0.66616  -0.13595   0.057122]\n",
            "Token: 'existence', Embedding: [ 0.29411  -0.5611    0.19299   0.045764 -0.38261 ]\n",
            "Token: 'and', Embedding: [0.20327  0.47348  0.050877 0.002103 0.060547]\n",
            "Token: 'consciousness', Embedding: [-0.94552  0.3193   0.262    0.41797  0.25382]\n",
            "Token: 'has', Embedding: [-0.30666  -0.08218  -0.32038  -0.11622   0.093608]\n",
            "Token: 'sprung', Embedding: [-0.002931  0.085132 -0.42228  -0.15877  -0.095795]\n",
            "Token: 'up', Embedding: [ 0.32335   0.10116  -0.7182   -0.098222  0.10265 ]\n",
            "Token: 'in', Embedding: [-0.10272  0.3041  -0.13577 -0.27979 -0.40926]\n",
            "Token: 'me', Embedding: [ 0.34605  0.33953 -0.01154  0.11595 -0.27834]\n",
            "Token: 'within', Embedding: [-0.15887  -0.014129  0.41397  -0.098472 -0.086403]\n",
            "Token: 'these', Embedding: [ 0.2408   0.46328 -0.18936 -0.16177  0.20385]\n",
            "Token: 'peeling', Embedding: [-0.54873 -0.4133  -0.10959  0.14457  0.51581]\n",
            "Token: 'walls…', Embedding: [0. 0. 0. 0. 0.]\n",
            "Token: 'and', Embedding: [0.20327  0.47348  0.050877 0.002103 0.060547]\n",
            "Token: 'what', Embedding: [ 0.39396    0.44185   -0.0042279 -0.044507  -0.13598  ]\n",
            "Token: 'is', Embedding: [ 0.32928   0.25526   0.26753  -0.084809  0.29764 ]\n",
            "Token: 'suffering', Embedding: [-0.31963  -0.18421  -0.20885   0.017626 -0.35361 ]\n",
            "Token: '?', Embedding: [ 0.39111   0.40186  -0.15055  -0.035758 -0.27055 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'am', Embedding: [-0.26604  0.4732   0.31872 -0.46331 -0.25759]\n",
            "Token: 'not', Embedding: [ 0.34303   0.4082   -0.023317 -0.36093   0.0526  ]\n",
            "Token: 'afraid', Embedding: [ 0.32403  -0.17814  -0.019409  0.056288 -0.43115 ]\n",
            "Token: 'of', Embedding: [ 0.052924  0.25427   0.31353  -0.35613   0.029629]\n",
            "Token: 'it', Embedding: [ 0.21632  0.21896  0.12569 -0.17436  0.17336]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'even', Embedding: [ 0.44802   0.16025  -0.23372  -0.054205 -0.067149]\n",
            "Token: 'if', Embedding: [0.56699  0.57946  0.18112  0.17006  0.074038]\n",
            "Token: 'it', Embedding: [ 0.21632  0.21896  0.12569 -0.17436  0.17336]\n",
            "Token: 'were', Embedding: [-0.29716  -0.025477 -0.43389  -0.52616  -0.14354 ]\n",
            "Token: 'beyond', Embedding: [ 0.19766  -0.11626  -0.23853  -0.046909 -0.12131 ]\n",
            "Token: 'reckoning', Embedding: [ 0.15398    0.15808    0.11411    0.0027507 -0.27107  ]\n",
            "Token: '.', Embedding: [ 0.12289   0.58037  -0.069635 -0.50288   0.10503 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'am', Embedding: [-0.26604  0.4732   0.31872 -0.46331 -0.25759]\n",
            "Token: 'not', Embedding: [ 0.34303   0.4082   -0.023317 -0.36093   0.0526  ]\n",
            "Token: 'afraid', Embedding: [ 0.32403  -0.17814  -0.019409  0.056288 -0.43115 ]\n",
            "Token: 'of', Embedding: [ 0.052924  0.25427   0.31353  -0.35613   0.029629]\n",
            "Token: 'it', Embedding: [ 0.21632  0.21896  0.12569 -0.17436  0.17336]\n",
            "Token: 'now', Embedding: [ 0.17821  -0.043552 -0.047699  0.20681   0.18358 ]\n",
            "Token: '.', Embedding: [ 0.12289   0.58037  -0.069635 -0.50288   0.10503 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'was', Embedding: [-0.1996  -0.27529 -0.21257 -0.71516 -0.1549 ]\n",
            "Token: 'afraid', Embedding: [ 0.32403  -0.17814  -0.019409  0.056288 -0.43115 ]\n",
            "Token: 'of', Embedding: [ 0.052924  0.25427   0.31353  -0.35613   0.029629]\n",
            "Token: 'it', Embedding: [ 0.21632  0.21896  0.12569 -0.17436  0.17336]\n",
            "Token: 'before…', Embedding: [0. 0. 0. 0. 0.]\n",
            "Token: 'and', Embedding: [0.20327  0.47348  0.050877 0.002103 0.060547]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'seem', Embedding: [ 0.46434  0.16059 -0.14039 -0.28814  0.24309]\n",
            "Token: 'to', Embedding: [ 0.57346  0.5417  -0.23477 -0.3624   0.4037 ]\n",
            "Token: 'have', Embedding: [-0.243    -0.014334 -0.38246  -0.21259  -0.095778]\n",
            "Token: 'such', Embedding: [ 0.42784  0.64209 -0.2957  -0.34537  0.39223]\n",
            "Token: 'strength', Embedding: [ 0.21762  -0.053931  0.059835 -0.036046  0.21309 ]\n",
            "Token: 'in', Embedding: [-0.10272  0.3041  -0.13577 -0.27979 -0.40926]\n",
            "Token: 'me', Embedding: [ 0.34605  0.33953 -0.01154  0.11595 -0.27834]\n",
            "Token: 'now', Embedding: [ 0.17821  -0.043552 -0.047699  0.20681   0.18358 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'that', Embedding: [ 0.14805   0.10875  -0.036278 -0.15386   0.37181 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'think', Embedding: [ 0.31806   0.20393  -0.12459  -0.021774 -0.021626]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'could', Embedding: [ 0.15973  0.1416  -0.2158  -0.22323  0.35053]\n",
            "Token: 'stand', Embedding: [ 0.22787  0.30175 -0.40343  0.13181 -0.33456]\n",
            "Token: 'anything', Embedding: [ 0.42697  0.18069 -0.11546 -0.30937 -0.1883 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'any', Embedding: [ 0.63113  0.43183  0.23103 -0.64909  0.23744]\n",
            "Token: 'suffering', Embedding: [-0.31963  -0.18421  -0.20885   0.017626 -0.35361 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'only', Embedding: [ 0.16347   0.17487  -0.13832  -0.18389   0.086837]\n",
            "Token: 'to', Embedding: [ 0.57346  0.5417  -0.23477 -0.3624   0.4037 ]\n",
            "Token: 'be', Embedding: [ 0.14336    0.32323   -0.0012141 -0.30418    0.032943 ]\n",
            "Token: 'able', Embedding: [ 0.42923  0.22089 -0.22135 -0.19993  0.23083]\n",
            "Token: 'to', Embedding: [ 0.57346  0.5417  -0.23477 -0.3624   0.4037 ]\n",
            "Token: 'say', Embedding: [ 0.13206   -0.0024709 -0.030482  -0.055545   0.025463 ]\n",
            "Token: 'and', Embedding: [0.20327  0.47348  0.050877 0.002103 0.060547]\n",
            "Token: 'to', Embedding: [ 0.57346  0.5417  -0.23477 -0.3624   0.4037 ]\n",
            "Token: 'repeat', Embedding: [ 0.48671   0.93464  -0.015418 -0.46056  -0.15226 ]\n",
            "Token: 'to', Embedding: [ 0.57346  0.5417  -0.23477 -0.3624   0.4037 ]\n",
            "Token: 'myself', Embedding: [ 0.26019  0.4785  -0.53881  0.22078 -0.51154]\n",
            "Token: 'every', Embedding: [ 0.55499   0.67268  -0.081677 -0.30823  -0.082756]\n",
            "Token: 'moment', Embedding: [ 0.31521  0.54651 -0.13988 -0.52135 -0.72683]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: '‘', Embedding: [ 0.33057    0.0058394 -0.0026677  0.28709   -0.2346   ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'exist.', Embedding: [0. 0. 0. 0. 0.]\n",
            "Token: '’', Embedding: [ 0.21752  -0.11792  -0.088778  0.33265  -0.23704 ]\n",
            "Token: 'in', Embedding: [-0.10272  0.3041  -0.13577 -0.27979 -0.40926]\n",
            "Token: 'thousands', Embedding: [ 0.16164 -0.44544 -0.20135  0.08019 -0.24991]\n",
            "Token: 'of', Embedding: [ 0.052924  0.25427   0.31353  -0.35613   0.029629]\n",
            "Token: 'agonies', Embedding: [-0.12813  -0.091163  0.15875   0.5773   -0.39179 ]\n",
            "Token: '—', Embedding: [ 0.23401  -0.003888 -0.015521  0.032232 -0.13994 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'exist', Embedding: [0.40714 0.26434 0.37526 0.01078 0.10304]\n",
            "Token: '.', Embedding: [ 0.12289   0.58037  -0.069635 -0.50288   0.10503 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: '’', Embedding: [ 0.21752  -0.11792  -0.088778  0.33265  -0.23704 ]\n",
            "Token: 'm', Embedding: [ 0.27704 -0.34803 -0.54378  0.2792   0.44356]\n",
            "Token: 'tormented', Embedding: [-0.087608 -0.27079   0.058618  0.59892  -0.22894 ]\n",
            "Token: 'on', Embedding: [-0.39374   0.55684  -0.35848  -0.67074   0.073665]\n",
            "Token: 'the', Embedding: [-0.071549  0.093459  0.023738 -0.090339  0.056123]\n",
            "Token: 'rack', Embedding: [0.47955 0.77247 0.13851 0.62758 0.16891]\n",
            "Token: '—', Embedding: [ 0.23401  -0.003888 -0.015521  0.032232 -0.13994 ]\n",
            "Token: 'but', Embedding: [ 0.24438   0.053703 -0.23639  -0.25545   0.18797 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'exist', Embedding: [0.40714 0.26434 0.37526 0.01078 0.10304]\n",
            "Token: '!', Embedding: [ 0.50368  0.7717  -0.49917 -0.11844 -0.31343]\n",
            "Token: 'though', Embedding: [ 0.19866   -0.095063  -0.10798   -0.11013    0.0052079]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'sit', Embedding: [ 0.65503   0.27148  -0.13185  -0.085942 -0.28181 ]\n",
            "Token: 'alone', Embedding: [ 0.35012   0.23944  -0.18498   0.065594 -0.13079 ]\n",
            "Token: 'on', Embedding: [-0.39374   0.55684  -0.35848  -0.67074   0.073665]\n",
            "Token: 'a', Embedding: [ 0.24169 -0.34534 -0.22307 -1.2907   0.25285]\n",
            "Token: 'pillar', Embedding: [-0.098824 -0.22158  -0.31766   0.65825  -0.50977 ]\n",
            "Token: '—', Embedding: [ 0.23401  -0.003888 -0.015521  0.032232 -0.13994 ]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'exist', Embedding: [0.40714 0.26434 0.37526 0.01078 0.10304]\n",
            "Token: '!', Embedding: [ 0.50368  0.7717  -0.49917 -0.11844 -0.31343]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'see', Embedding: [ 0.57638   0.59676  -0.073165  0.11275   0.2183  ]\n",
            "Token: 'the', Embedding: [-0.071549  0.093459  0.023738 -0.090339  0.056123]\n",
            "Token: 'sun', Embedding: [ 0.45253  0.35506  0.3146  -0.20477  0.66384]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'and', Embedding: [0.20327  0.47348  0.050877 0.002103 0.060547]\n",
            "Token: 'if', Embedding: [0.56699  0.57946  0.18112  0.17006  0.074038]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'don', Embedding: [-0.86746   0.47017  -0.50487  -0.35623  -0.064959]\n",
            "Token: '’', Embedding: [ 0.21752  -0.11792  -0.088778  0.33265  -0.23704 ]\n",
            "Token: 't', Embedding: [ 0.42596 -0.18836 -0.65115  0.72988  0.86216]\n",
            "Token: 'see', Embedding: [ 0.57638   0.59676  -0.073165  0.11275   0.2183  ]\n",
            "Token: 'the', Embedding: [-0.071549  0.093459  0.023738 -0.090339  0.056123]\n",
            "Token: 'sun', Embedding: [ 0.45253  0.35506  0.3146  -0.20477  0.66384]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'i', Embedding: [ 0.26805  0.36032 -0.332   -0.54642 -0.50451]\n",
            "Token: 'know', Embedding: [ 0.43934  0.28488  0.1432  -0.11852 -0.27219]\n",
            "Token: 'it', Embedding: [ 0.21632  0.21896  0.12569 -0.17436  0.17336]\n",
            "Token: '’', Embedding: [ 0.21752  -0.11792  -0.088778  0.33265  -0.23704 ]\n",
            "Token: 's', Embedding: [ 0.18209  0.88297 -0.49805  0.53137 -0.36084]\n",
            "Token: 'there', Embedding: [ 0.66193   0.16192  -0.090129 -0.59287   0.15391 ]\n",
            "Token: '.', Embedding: [ 0.12289   0.58037  -0.069635 -0.50288   0.10503 ]\n",
            "Token: 'and', Embedding: [0.20327  0.47348  0.050877 0.002103 0.060547]\n",
            "Token: 'there', Embedding: [ 0.66193   0.16192  -0.090129 -0.59287   0.15391 ]\n",
            "Token: '’', Embedding: [ 0.21752  -0.11792  -0.088778  0.33265  -0.23704 ]\n",
            "Token: 's', Embedding: [ 0.18209  0.88297 -0.49805  0.53137 -0.36084]\n",
            "Token: 'a', Embedding: [ 0.24169 -0.34534 -0.22307 -1.2907   0.25285]\n",
            "Token: 'whole', Embedding: [ 0.24418 -0.18986  0.50628  0.50667 -0.21567]\n",
            "Token: 'life', Embedding: [ 0.34098   0.41888  -0.31878   0.031399  0.047223]\n",
            "Token: 'in', Embedding: [-0.10272  0.3041  -0.13577 -0.27979 -0.40926]\n",
            "Token: 'that', Embedding: [ 0.14805   0.10875  -0.036278 -0.15386   0.37181 ]\n",
            "Token: ',', Embedding: [ 0.17651    0.29208   -0.0020768 -0.37523    0.0049139]\n",
            "Token: 'in', Embedding: [-0.10272  0.3041  -0.13577 -0.27979 -0.40926]\n",
            "Token: 'knowing', Embedding: [ 0.35481 -0.05835  0.36272 -0.12141 -0.53815]\n",
            "Token: 'that', Embedding: [ 0.14805   0.10875  -0.036278 -0.15386   0.37181 ]\n",
            "Token: 'the', Embedding: [-0.071549  0.093459  0.023738 -0.090339  0.056123]\n",
            "Token: 'sun', Embedding: [ 0.45253  0.35506  0.3146  -0.20477  0.66384]\n",
            "Token: 'is', Embedding: [ 0.32928   0.25526   0.26753  -0.084809  0.29764 ]\n",
            "Token: 'there', Embedding: [ 0.66193   0.16192  -0.090129 -0.59287   0.15391 ]\n",
            "Token: '.', Embedding: [ 0.12289   0.58037  -0.069635 -0.50288   0.10503 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We see that the nltk considers the punctuation marks as tokens and Glove only has vectors corrosponding to words and not the pinctuation marks, and eventough we have asked to return a 0 vector for the words not in the glove embeddings we are getting a specific vector value**"
      ],
      "metadata": {
        "id": "-RoW4SM5XIZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code with word index from tokens"
      ],
      "metadata": {
        "id": "KVA9ccvTa6go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a word index (mapping words to unique indices)\n",
        "word_index = {word: i for i, word in enumerate(set(tokens), start=0)}\n",
        "print(f\"Word Index: {word_index}\")  # Debugging: See word index\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvFsH_gRbEEQ",
        "outputId": "157d8731-1104-4475-ce1f-9acba5771783"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index: {'.': 0, 'rack': 1, 'pillar': 2, 'sun': 3, 't': 4, 'believe': 5, 'thousands': 6, 'full': 7, 'life': 8, 'such': 9, 'even': 10, 'knowing': 11, 'live': 12, 'suffering': 13, 'yes': 14, 'sprung': 15, 'consciousness': 16, 'anything': 17, 'you': 18, 'for': 19, '‘': 20, 'began': 21, 'reckoning': 22, 'i': 23, 'be': 24, 'stand': 25, 'alexey': 26, 'me': 27, 'on': 28, 'has': 29, 'up': 30, 'agonies': 31, 'sit': 32, 'though': 33, '?': 34, 'again': 35, 'peeling': 36, 'not': 37, 'was': 38, 'there': 39, 'know': 40, 'he': 41, ',': 42, 'exist': 43, '“': 44, 'seem': 45, 'these': 46, 'but': 47, 'see': 48, 'if': 49, 'were': 50, 'repeat': 51, 'wouldn': 52, 'any': 53, 'beyond': 54, 'every': 55, 'able': 56, 'want': 57, 'a': 58, 'of': 59, 'existence': 60, 'within': 61, '”': 62, 'don': 63, '’': 64, 'have': 65, 'strength': 66, 's': 67, '—': 68, 'myself': 69, 'that': 70, 'think': 71, 'in': 72, 'how': 73, 'it': 74, 'walls…': 75, 'exist.': 76, 'underground': 77, 'thirst': 78, 'am': 79, 'now': 80, 'afraid': 81, 'is': 82, 'before…': 83, 'the': 84, 'and': 85, 'tormented': 86, '!': 87, 'm': 88, 'to': 89, 'only': 90, 'what': 91, 'whole': 92, 'say': 93, 'alone': 94, 'moment': 95, 'could': 96}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the embedding matrix using the word index\n",
        "embedding_dim = 200  # Set according to your GloVe vectors\n",
        "embedding_matrix = get_embedding_matrix(word_index, glove_embeddings, embedding_dim)"
      ],
      "metadata": {
        "id": "eBYUnhK2bCmJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "3ckhFDa2ETiy"
      },
      "outputs": [],
      "source": [
        "# Map words to GloVe embeddings\n",
        "def get_embedding_matrix(word_index, glove_embeddings, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # +1 for padding or unknowns\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = glove_embeddings.get(word)  # Lookup the embedding\n",
        "        if embedding_vector is not None:\n",
        "            print(f\"Embedding found for word '{word}'\")  # Debugging: Word found in GloVe\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        else:\n",
        "            print(f\"No embedding for word '{word}', assigning zero vector\")  # Debugging: Word not found\n",
        "    return embedding_matrix\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the GloVe embeddings\n",
        "glove_file_path = '/content/glove.6B.100d.txt'  # Update with your file path\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path)\n"
      ],
      "metadata": {
        "id": "_uzJ5e4iM9er"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the embedding matrix\n",
        "embedding_dim = 100  # Set according to your GloVe vectors\n",
        "embedding_matrix = get_embedding_matrix(word_index, glove_embeddings, embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kqpubh-NMB5",
        "outputId": "9de815e4-9592-4865-cfc7-39c6215b7584"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding found for word '.'\n",
            "Embedding found for word 'rack'\n",
            "Embedding found for word 'pillar'\n",
            "Embedding found for word 'sun'\n",
            "Embedding found for word 't'\n",
            "Embedding found for word 'believe'\n",
            "Embedding found for word 'thousands'\n",
            "Embedding found for word 'full'\n",
            "Embedding found for word 'life'\n",
            "Embedding found for word 'such'\n",
            "Embedding found for word 'even'\n",
            "Embedding found for word 'knowing'\n",
            "Embedding found for word 'live'\n",
            "Embedding found for word 'suffering'\n",
            "Embedding found for word 'yes'\n",
            "Embedding found for word 'sprung'\n",
            "Embedding found for word 'consciousness'\n",
            "Embedding found for word 'anything'\n",
            "Embedding found for word 'you'\n",
            "Embedding found for word 'for'\n",
            "Embedding found for word '‘'\n",
            "Embedding found for word 'began'\n",
            "Embedding found for word 'reckoning'\n",
            "Embedding found for word 'i'\n",
            "Embedding found for word 'be'\n",
            "Embedding found for word 'stand'\n",
            "Embedding found for word 'alexey'\n",
            "Embedding found for word 'me'\n",
            "Embedding found for word 'on'\n",
            "Embedding found for word 'has'\n",
            "Embedding found for word 'up'\n",
            "Embedding found for word 'agonies'\n",
            "Embedding found for word 'sit'\n",
            "Embedding found for word 'though'\n",
            "Embedding found for word '?'\n",
            "Embedding found for word 'again'\n",
            "Embedding found for word 'peeling'\n",
            "Embedding found for word 'not'\n",
            "Embedding found for word 'was'\n",
            "Embedding found for word 'there'\n",
            "Embedding found for word 'know'\n",
            "Embedding found for word 'he'\n",
            "Embedding found for word ','\n",
            "Embedding found for word 'exist'\n",
            "Embedding found for word '“'\n",
            "Embedding found for word 'seem'\n",
            "Embedding found for word 'these'\n",
            "Embedding found for word 'but'\n",
            "Embedding found for word 'see'\n",
            "Embedding found for word 'if'\n",
            "Embedding found for word 'were'\n",
            "Embedding found for word 'repeat'\n",
            "Embedding found for word 'wouldn'\n",
            "Embedding found for word 'any'\n",
            "Embedding found for word 'beyond'\n",
            "Embedding found for word 'every'\n",
            "Embedding found for word 'able'\n",
            "Embedding found for word 'want'\n",
            "Embedding found for word 'a'\n",
            "Embedding found for word 'of'\n",
            "Embedding found for word 'existence'\n",
            "Embedding found for word 'within'\n",
            "Embedding found for word '”'\n",
            "Embedding found for word 'don'\n",
            "Embedding found for word '’'\n",
            "Embedding found for word 'have'\n",
            "Embedding found for word 'strength'\n",
            "Embedding found for word 's'\n",
            "Embedding found for word '—'\n",
            "Embedding found for word 'myself'\n",
            "Embedding found for word 'that'\n",
            "Embedding found for word 'think'\n",
            "Embedding found for word 'in'\n",
            "Embedding found for word 'how'\n",
            "Embedding found for word 'it'\n",
            "No embedding for word 'walls…', assigning zero vector\n",
            "No embedding for word 'exist.', assigning zero vector\n",
            "Embedding found for word 'underground'\n",
            "Embedding found for word 'thirst'\n",
            "Embedding found for word 'am'\n",
            "Embedding found for word 'now'\n",
            "Embedding found for word 'afraid'\n",
            "Embedding found for word 'is'\n",
            "No embedding for word 'before…', assigning zero vector\n",
            "Embedding found for word 'the'\n",
            "Embedding found for word 'and'\n",
            "Embedding found for word 'tormented'\n",
            "Embedding found for word '!'\n",
            "Embedding found for word 'm'\n",
            "Embedding found for word 'to'\n",
            "Embedding found for word 'only'\n",
            "Embedding found for word 'what'\n",
            "Embedding found for word 'whole'\n",
            "Embedding found for word 'say'\n",
            "Embedding found for word 'alone'\n",
            "Embedding found for word 'moment'\n",
            "Embedding found for word 'could'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the embedding for a punctuation mark (comma)\n",
        "comma_embedding = embedding_matrix[word_index[',']]\n",
        "print(f\"Embedding for ',': {comma_embedding}\")  # Should return a zero vector if not found"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eoHggrwOsfs",
        "outputId": "a8928e98-4a31-4e6d-9349-e98c994a6add"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for ',': [-0.10767     0.11053     0.59811997 -0.54360998  0.67395997  0.10663\n",
            "  0.038867    0.35481     0.06351    -0.094189    0.15786    -0.81664997\n",
            "  0.14172     0.21939     0.58504999 -0.52157998  0.22782999 -0.16642\n",
            " -0.68228     0.35870001  0.42568001  0.19021     0.91962999  0.57555002\n",
            "  0.46184999  0.42363    -0.095399   -0.42749    -0.16566999 -0.056842\n",
            " -0.29595     0.26036999 -0.26605999 -0.070404   -0.27662     0.15820999\n",
            "  0.69825     0.43081     0.27952    -0.45436999 -0.33801001 -0.58183998\n",
            "  0.22363999 -0.57779998 -0.26862001 -0.20424999  0.56393999 -0.58524001\n",
            " -0.14365    -0.64218003  0.0054697  -0.35247999  0.16162001  1.1796\n",
            " -0.47674    -2.75530005 -0.1321     -0.047729    1.06550002  1.10339999\n",
            " -0.2208      0.18669     0.13177     0.15117     0.71310002 -0.35214999\n",
            "  0.91347998  0.61782998  0.70991999  0.23954999 -0.14571001 -0.37858999\n",
            " -0.045959   -0.47367999  0.2385      0.20536    -0.18996     0.32506999\n",
            " -1.11119998 -0.36341     0.98679    -0.084776   -0.54008001  0.11726\n",
            " -1.0194     -0.24424     0.12771     0.013884    0.080374   -0.35414001\n",
            "  0.34951001 -0.72259998  0.37549001  0.44409999 -0.99058998  0.61214\n",
            " -0.35111001 -0.83155     0.45293     0.082577  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that we are still geting a nonzero vector for ','"
      ],
      "metadata": {
        "id": "dbwzNczVOv_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the glove embediings whether it has the embeddings for ','\n",
        "print(list(glove_embeddings.keys())[:20])  # Print the first 20 keys in GloVe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnB3hahmPLsv",
        "outputId": "8993e83b-e863-4af5-ebba-9f5df82c0fd0"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for', '-', 'that', 'on', 'is', 'was', 'said', 'with', 'he', 'as']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference**\n",
        "\n",
        "So its confirmed that the glove embedding vocabulary that we have used here has the embeddings of the punctuation marks as well as words."
      ],
      "metadata": {
        "id": "UfVTh0JIPZE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forcing the glove emedding vocabulary to return 0 vectors for the punctuation marks"
      ],
      "metadata": {
        "id": "oLI2pk5LP7yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Force punctuation (like commas) to have zero vectors\n",
        "punctuation = [',', '.', '!', '?', ':', ';']  # Add any other punctuation if needed\n",
        "\n",
        "for punc in punctuation:\n",
        "    if punc in word_index:\n",
        "        embedding_matrix[word_index[punc]] = np.zeros(embedding_dim)"
      ],
      "metadata": {
        "id": "8kj_aapmQI0s"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comma_embedding = embedding_matrix[word_index[',']]\n",
        "print(f\"Embedding for ',': {comma_embedding}\")  # Should return a zero vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_gGmwPxQSA4",
        "outputId": "0756a133-2bf1-472e-9346-9e38b45685ff"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for ',': [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0.]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMf4u2m1pcaSZa723/Eojnz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}