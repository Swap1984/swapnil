{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QYRnowpl8ySC"
      ],
      "authorship_tag": "ABX9TyOE70rEKSfb3C3Y/sXuaTQa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swap1984/swapnil/blob/main/Assignment_Bag_of_words_multiple_documents_at_same_time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tJq4NYTuLnD0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for Bag of Words embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "QYRnowpl8ySC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Bag of Words (BoW): Method of Operation**\n",
        "\n",
        "Method of Operation:\n",
        "\n",
        "The Bag of Words (BoW) model is a simple and widely used technique for text representation. It converts a text into a matrix where:\n",
        "\n",
        "Rows represent documents (or sentences, or any text unit).\n",
        "Columns represent unique words (or bigrams, trigrams, etc.).\n",
        "The values in the matrix represent the frequency (or count) of each word in each document.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Tokenization: The text is broken down into individual words (or tokens).\n",
        "Vocabulary Building: A vocabulary (set of unique words) is created from the tokens.\n",
        "\n",
        "Frequency Count: Each word in the vocabulary is counted for its occurrences in the document.\n",
        "\n",
        "**Advantages of BoW:**\n",
        "\n",
        "Simplicity: BoW is easy to understand and implement.\n",
        "\n",
        "Efficient for Smaller Datasets: Works well for small text data, especially when there's not much need for understanding context or word order.\n",
        "\n",
        "No Need for Pre-trained Models: It doesn’t require pre-trained word embeddings, so it's quick to generate.\n",
        "\n",
        "**Disadvantages of BoW:**\n",
        "\n",
        "Ignores Context: It doesn't capture the context or word order (e.g., \"dog bites man\" and \"man bites dog\" will be treated similarly).\n",
        "\n",
        "Sparsity: As vocabulary size increases, the matrix becomes sparse (many 0s) for larger datasets, which can increase computational cost.\n",
        "\n",
        "High Dimensionality: BoW creates a large feature space, which can make training machine learning models harder.\n",
        "\n",
        "Assumes All Words are Equally Important: It only considers the frequency of words, ignoring their importance or relevance.\n",
        "\n",
        "**Applications of BoW:**\n",
        "\n",
        "Text Classification: Frequently used in classification problems (spam detection, sentiment analysis) where contextual information isn't crucial.\n",
        "Information Retrieval: Search engines often use BoW for indexing documents and retrieving based on word matches.\n",
        "\n",
        "Topic Modeling: It can be a base input for algorithms like LDA (Latent Dirichlet Allocation) to find topics in a corpus."
      ],
      "metadata": {
        "id": "3XeVQJJ1M6PX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialising Libraries for  preprocessing the data for emmbedding\n"
      ],
      "metadata": {
        "id": "ZrS8n8EHAbyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import string #This module is used to remove punctuation from the text.\n",
        "import re #This regular expressions module is useful for operations like replacing repeated characters.\n",
        "import nltk # Natural language tool kit library\n",
        "from nltk.corpus import stopwords #  Provides a list of common English stopwords (e.g., \"the\", \"is\") that are generally not informative in text analysis.\n",
        "from nltk.tokenize import word_tokenize # to Tokenize a string into individual words.\n",
        "from nltk.stem import WordNetLemmatizer # to Convert words to their base (dictionary) form using lemmatization.\n",
        "from sklearn.feature_extraction.text import CountVectorizer #to convert a collection of text documents into a matrix of token counts."
      ],
      "metadata": {
        "id": "4knDdxQk8sdK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Consider the following data with each sentence being treated as a separate document\n",
        "data = [\"”Yes, life is full, there is life even underground,” he began again.\" ,\n",
        "\"“You wouldn’t believe, Alexey, how I want to live now, what a thirst for existence and consciousness has sprung up in me within these peeling walls…\",\n",
        "\" And what is suffering? I am not afraid of it, even if it were beyond reckoning.\",\n",
        "\" I am not afraid of it now.\",\n",
        "\" I was afraid of it before… And I seem to have such strength in me now, that I think I could stand anything, any suffering, only to be able to say and to repeat to myself every moment, ‘I exist.’\",\n",
        "\"In thousands of agonies—I exist.\",\n",
        "\" I’m tormented on the rack — but I exist! Though I sit alone on a pillar — I exist! I see the sun, and if I don’t see the sun, I know it’s there. And there’s a whole life in that, in knowing that the sun is there.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "CpjVCi6q8pos"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the data"
      ],
      "metadata": {
        "id": "ZV6pYjaFczmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords if not already present\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stopwords list\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to preprocess text: lowercasing, removing punctuation, and tokenizing\n",
        "def preprocess(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in text.split() if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocess all sentences\n",
        "preprocessed_data = [preprocess(sentence) for sentence in data]\n",
        "print(preprocessed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEFenDkYsDHj",
        "outputId": "713c42d1-8890-4699-ab9a-2d6eeb0e51ff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yes life full life even underground began', 'wouldnt believe alexey want live thirst existence consciousness sprung within peeling walls', 'suffering afraid even beyond reckoning', 'afraid', 'afraid seem strength think could stand anything suffering able say repeat every moment exist', 'thousands agoniesi exist', 'im tormented rack exist though sit alone pillar exist see sun dont see sun know theres whole life knowing sun']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CountVectorizer (BoW)\n",
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "GdjT02dtsjYt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and transform the data to create the BoW matrix\n",
        "bow_matrix = vectorizer.fit_transform(preprocessed_data)\n",
        "bow_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EqSdyS3spup",
        "outputId": "18c2d4f3-ec17-493e-b6f6-91995af945b2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<7x50 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 57 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert matrix to an array\n",
        "bow_array = bow_matrix.toarray()\n",
        "bow_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD_FjYOqswr5",
        "outputId": "b611da41-c86b-4fba-d07d-2d656a9f05fb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "        0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "        1, 1, 0, 1, 1, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0,\n",
              "        0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 3, 1, 0, 0, 1, 0, 1, 0,\n",
              "        0, 0, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature names (i.e., unique words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "feature_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2ArBYbgs7_k",
        "outputId": "095b0c50-2775-4cbe-9550-91f57513ebd1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['able', 'afraid', 'agoniesi', 'alexey', 'alone', 'anything',\n",
              "       'began', 'believe', 'beyond', 'consciousness', 'could', 'dont',\n",
              "       'even', 'every', 'exist', 'existence', 'full', 'im', 'know',\n",
              "       'knowing', 'life', 'live', 'moment', 'peeling', 'pillar', 'rack',\n",
              "       'reckoning', 'repeat', 'say', 'see', 'seem', 'sit', 'sprung',\n",
              "       'stand', 'strength', 'suffering', 'sun', 'theres', 'think',\n",
              "       'thirst', 'though', 'thousands', 'tormented', 'underground',\n",
              "       'walls', 'want', 'whole', 'within', 'wouldnt', 'yes'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the BoW matrix and feature names\n",
        "print(\"Feature Names (Vocabulary):\")\n",
        "print(feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cRqFPdLtEHk",
        "outputId": "51d4f294-1503-4195-ce9c-c1a5bf2fbc68"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names (Vocabulary):\n",
            "['able' 'afraid' 'agoniesi' 'alexey' 'alone' 'anything' 'began' 'believe'\n",
            " 'beyond' 'consciousness' 'could' 'dont' 'even' 'every' 'exist'\n",
            " 'existence' 'full' 'im' 'know' 'knowing' 'life' 'live' 'moment' 'peeling'\n",
            " 'pillar' 'rack' 'reckoning' 'repeat' 'say' 'see' 'seem' 'sit' 'sprung'\n",
            " 'stand' 'strength' 'suffering' 'sun' 'theres' 'think' 'thirst' 'though'\n",
            " 'thousands' 'tormented' 'underground' 'walls' 'want' 'whole' 'within'\n",
            " 'wouldnt' 'yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBag of Words Matrix:\")\n",
        "print(bow_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsJu0z46tM0v",
        "outputId": "38e01362-43b4-4eb6-bc2f-6d392cb5d670"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bag of Words Matrix:\n",
            "[[0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            " [0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 1 0 0 0 0 1 1 0 1 1 0]\n",
            " [0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 1\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 1 0 0 2 0 0 1 1 1 1 0 0 0 1 1 0 0 0 2 0 1 0 0 0 0\n",
            "  3 1 0 0 1 0 1 0 0 0 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the number of unique words (vocabulary size)\n",
        "print(f\"Number of unique words in vocabulary: {len(feature_names)}\")\n",
        "\n",
        "# Output the shape of the BoW matrix (number of rows, number of columns/features)\n",
        "print(f\"Shape of the BoW matrix: {bow_array.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G6wyd67zmcJ",
        "outputId": "451a3de2-2706-4677-90a9-2b01588ad83b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in vocabulary: 50\n",
            "Shape of the BoW matrix: (7, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference and Anaysis**\n",
        "\n",
        "As we see the BoW matrix is very sparse and some worrd like 'exist' appear in three documents. also words like 'again'are not considered in the corpus because its very rare and occures only in one doc."
      ],
      "metadata": {
        "id": "jAFR8QBrvdwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** the word 'would't' and 'again' both are rare 'again was droped from the corpus and 'would't' was kept. why this happened?"
      ],
      "metadata": {
        "id": "8PHIDUSy16qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(\"again\" in stopwords.words('english'))  # Check if \"again\" is a stopword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCKH6kgR27Eu",
        "outputId": "cc4ae8c7-c60b-4831-a8b0-9870cd21a6a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**:\n",
        "Thus we see that\n",
        "\n",
        "\"again\" was dropped: This is because \"again\" is included in NLTK's English stopwords list.\n",
        "\n",
        "\"wouldn't\" was kept: If contractions are not expanded and stopword removal is applied, \"wouldn't\" remains as a single token and is not in the stopwords list."
      ],
      "metadata": {
        "id": "6YnGf1h33Ghx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kjxz_oU_5JS1"
      }
    }
  ]
}